{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from langdetect import detect\n",
    "import re\n",
    "import string\n",
    "#from project_functions import *\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import sentiment_mod as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'API Tweeter.ipynb'   featuresets.pickle\t      Text_Mining_MASTER\r\n",
      " backup\t\t     'Machine Learning'\t\t      tweets_df.csv\r\n",
      "'Deep Learning'      'PLaying with tutorials.ipynb'   tweets_jan2019\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/jovyan/work/2_Semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>@ #1,  Bitcoin  with unit price of $3,742.7, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>Learn it. 2018 Sees  Bitcoin ’s Lowest Average...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>仮想通貨の時価総額  $125,622,003,150   BTC  価格:$3734.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>IAM Platform Curated Retweet:  Via:  https:// ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>Bitcoin  - BTC Price: $3,742.70 Change in 1h: ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime                                               text\n",
       "0  2019-01-01 00:00:00  @ #1,  Bitcoin  with unit price of $3,742.7, m...\n",
       "1  2019-01-01 00:00:00  Learn it. 2018 Sees  Bitcoin ’s Lowest Average...\n",
       "2  2019-01-01 00:00:01  仮想通貨の時価総額  $125,622,003,150   BTC  価格:$3734.04...\n",
       "3  2019-01-01 00:00:01  IAM Platform Curated Retweet:  Via:  https:// ...\n",
       "4  2019-01-01 00:00:01  Bitcoin  - BTC Price: $3,742.70 Change in 1h: ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_tweets = open(\"/home/jovyan/work/2_Semester/tweets_jan2019\", \"rb\")\n",
    "example_tweets = pickle.load(pickle_tweets)\n",
    "example_tweets = example_tweets[[\"datetime\",\"text\"]]\n",
    "example_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweets = example_tweets.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter on English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english(dataframe):\n",
    "    blanco = \"blanco\"\n",
    "    text_column = dataframe[\"text\"]\n",
    "    \n",
    "    # Create a list saving all the languages of the tweets\n",
    "    language_list =[]\n",
    "\n",
    "    for i in text_column:\n",
    "\n",
    "        try:\n",
    "            language = detect(i)\n",
    "            language_list.append(language)\n",
    "        except:\n",
    "            language_list.append(blanco)    \n",
    "    \n",
    "    dataframe[\"Language\"] = language_list\n",
    "    \n",
    "    return dataframe.loc[dataframe['Language'] == \"en\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = filter_english(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removal_function(dataframe):\n",
    "    new_text = []\n",
    "    text_column = dataframe[\"text\"]\n",
    "    for i in text_column:\n",
    "        y = i\n",
    "\n",
    "        y = re.sub(r\"@[A-Z-a-z-0-9_.]+\",\"\", y) #remove users with@\n",
    "        y = y.replace(\"\\n\",\" \") # remove enters\n",
    "        y= re.sub(r\"http\\S+\",\"\",y) # removes links\n",
    "        y= re.sub(\"\\s+\",\" \",y)  #removes more one spaces\n",
    "        y= re.sub(r\"&(amp;)\", \"&\", y) # removes and in html format\n",
    "        y = re.sub(r\"[0-9]\",\"\",y) #remove numbers\n",
    "        y=re.sub(r\"(.+?)\\1+\",r\"\\1\",y) #remove repeted letters\n",
    "        y= re.sub(\"\\s+\",\" \",y) #remove more one space\n",
    "\n",
    "        i = y\n",
    "        new_text.append(i)\n",
    "        \n",
    "    dataframe[\"text\"] = new_text\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = removal_function(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, Remove Stopwords, Lemmatize, Stemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataframe(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_text = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        i = i.lower()\n",
    "        i = RegexpTokenizer(r'\\w+').tokenize(i)\n",
    "        new_text.append(i)\n",
    "        \n",
    "    text_column = new_text\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = tokenize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_dataframe(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_words = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_text = [j for j in i if not j in stop_words]\n",
    "        new_words.append(stop_text)\n",
    "    \n",
    "    text_column = new_words\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = remove_stopwords_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_dataframe(dataframe):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_words = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        lemma = [wordnet.lemmatize(token) for token in i]\n",
    "        new_words.append(lemma)\n",
    "        \n",
    "    text_column = new_words\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweets = lemmatize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmatize_dataframe(dataframe):\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_words= []\n",
    "    \n",
    "    for i in text_column:\n",
    "        stemmed = [stemmer.stem(token) for token in i]\n",
    "        new_words.append(stemmed)\n",
    "    \n",
    "    text_column = new_words\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweets = stemmatize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize_dataframe(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_text = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        i = \" \".join(i)\n",
    "        new_text.append(i)\n",
    "        \n",
    "    text_column = new_text\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = untokenize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_textblob(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    sentiment_textblob_list = []\n",
    "\n",
    "    for i in text_column:\n",
    "        sentiment_value, polarity = s.sentiment_textblob(i)#sentiment_textblob(i)\n",
    "        sentiment_textblob_list.append(sentiment_value)\n",
    "\n",
    "    dataframe[\"sentiment_textblob\"] = sentiment_textblob_list\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = add_sentiment_textblob(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_nltk(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    sentiment_nltk_list = []\n",
    "\n",
    "    for i in text_column:\n",
    "        sentiment_value, polarity = s.sentiment_nltk(i)#sentiment_textblob(i)\n",
    "        sentiment_nltk_list.append(sentiment_value)\n",
    "\n",
    "    dataframe[\"sentiment_nltk\"] = sentiment_nltk_list\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = add_sentiment_nltk(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment own trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_own(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    sentiment_own_list = []\n",
    "\n",
    "    for i in text_column:\n",
    "        sentiment_value, polarity = s.sentiment(i)#sentiment_textblob(i)\n",
    "        if polarity < 0.75:\n",
    "            sentiment_own_list.append(\"neutral\")\n",
    "        else:\n",
    "            sentiment_own_list.append(sentiment_value)\n",
    "\n",
    "    dataframe[\"sentiment_own_classifiers\"] = sentiment_own_list\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = add_sentiment_own(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph sentiment & BTC time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "example_tweets_graph = copy.deepcopy(example_tweets)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweets_graph.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "example_tweets_graph[\"datetime\"] = pd.to_datetime(example_tweets_graph[\"datetime\"])\n",
    "example_tweets_graph = example_tweets_graph.set_index(\"datetime\")\n",
    "example_tweets_graph[\"hour\"] = example_tweets_graph.index.hour"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweets_graph.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pnn_counts_textblob = example_tweets_graph.groupby([\"hour\", \"sentiment_textblob\"])[\"text\"].count()\n",
    "pnn_counts_textblob = pnn_counts_textblob.to_frame()\n",
    "pnn_counts_textblob = pnn_counts_textblob.reset_index()\n",
    "pnn_counts_textblob = pnn_counts_textblob.rename(columns= {\"text\":\"textblob_count\"})\n",
    "\n",
    "pnn_counts_nltk = example_tweets_graph.groupby([\"hour\", \"sentiment_nltk\"])[\"text\"].count()\n",
    "pnn_counts_nltk = pnn_counts_nltk.to_frame()\n",
    "pnn_counts_nltk = pnn_counts_nltk.reset_index()\n",
    "pnn_counts_nltk = pnn_counts_nltk.rename(columns= {\"text\":\"nltk_count\"})\n",
    "\n",
    "pnn_counts_own = example_tweets_graph.groupby([\"hour\", \"sentiment_own_classifiers\"])[\"text\"].count()\n",
    "pnn_counts_own = pnn_counts_own.to_frame()\n",
    "pnn_counts_own = pnn_counts_own.reset_index()\n",
    "pnn_counts_own = pnn_counts_own.rename(columns= {\"text\":\"own_classifier_count\"})\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "textblob_count = pnn_counts_textblob[\"textblob_count\"].tolist()\n",
    "own_classifier_count = pnn_counts_own[\"own_classifier_count\"].tolist()\n",
    "pnn_counts_nltk[\"textblob_count\"] = textblob_count\n",
    "pnn_counts_nltk[\"own_classifier_count\"] = own_classifier_count\n",
    "\n",
    "pnn_counts = pnn_counts_nltk\n",
    "pnn_counts = pnn_counts.rename(columns= {\"sentiment_nltk\":\"sentiment\"})\n",
    "pnn_counts.head(9)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,2,3, figsize=(45,15))\n",
    "sns.lineplot(x=\"hour\", y=\"textblob_count\", hue=\"sentiment\", data=pnn_counts, ax=ax[0])\n",
    "sns.lineplot(x=\"hour\", y=\"nltk_count\", hue=\"sentiment\", data=pnn_counts, ax=ax[1])\n",
    "sns.lineplot(x=\"hour\", y=\"own_classifier_count\", hue=\"sentiment\", data=pnn_counts, ax=ax[2])\n",
    "#plt.legend(loc=\"upper left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
