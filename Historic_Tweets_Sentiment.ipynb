{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from langdetect import detect\n",
    "import re\n",
    "import string\n",
    "#from project_functions import *\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import sentiment_mod as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'API Tweeter.ipynb'   featuresets.pickle\t      Text_Mining_MASTER\r\n",
      " backup\t\t     'Machine Learning'\t\t      tweets_df.csv\r\n",
      "'Deep Learning'      'PLaying with tutorials.ipynb'   tweets_jan2019\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/jovyan/work/2_Semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>@ #1,  Bitcoin  with unit price of $3,742.7, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>Learn it. 2018 Sees  Bitcoin ’s Lowest Average...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>仮想通貨の時価総額  $125,622,003,150   BTC  価格:$3734.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>IAM Platform Curated Retweet:  Via:  https:// ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>Bitcoin  - BTC Price: $3,742.70 Change in 1h: ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime                                               text\n",
       "0  2019-01-01 00:00:00  @ #1,  Bitcoin  with unit price of $3,742.7, m...\n",
       "1  2019-01-01 00:00:00  Learn it. 2018 Sees  Bitcoin ’s Lowest Average...\n",
       "2  2019-01-01 00:00:01  仮想通貨の時価総額  $125,622,003,150   BTC  価格:$3734.04...\n",
       "3  2019-01-01 00:00:01  IAM Platform Curated Retweet:  Via:  https:// ...\n",
       "4  2019-01-01 00:00:01  Bitcoin  - BTC Price: $3,742.70 Change in 1h: ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_tweets = open(\"/home/jovyan/work/2_Semester/tweets_jan2019\", \"rb\")\n",
    "example_tweets = pickle.load(pickle_tweets)\n",
    "example_tweets = example_tweets[[\"datetime\",\"text\"]]\n",
    "example_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = example_tweets.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter on English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english(dataframe):\n",
    "    blanco = \"blanco\"\n",
    "    text_column = dataframe[\"text\"]\n",
    "    \n",
    "    # Create a list saving all the languages of the tweets\n",
    "    language_list =[]\n",
    "\n",
    "    for i in text_column:\n",
    "\n",
    "        try:\n",
    "            language = detect(i)\n",
    "            language_list.append(language)\n",
    "        except:\n",
    "            language_list.append(blanco)    \n",
    "    \n",
    "    dataframe[\"Language\"] = language_list\n",
    "    \n",
    "    return dataframe.loc[dataframe['Language'] == \"en\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = filter_english(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removal_function(dataframe):\n",
    "    new_text = []\n",
    "    text_column = dataframe[\"text\"]\n",
    "    for i in text_column:\n",
    "        y = i\n",
    "\n",
    "        y = re.sub(r\"@[A-Z-a-z-0-9_.]+\",\"\", y) #remove users with@\n",
    "        y = y.replace(\"\\n\",\" \") # remove enters\n",
    "        y= re.sub(r\"http\\S+\",\"\",y) # removes links\n",
    "        y= re.sub(\"\\s+\",\" \",y)  #removes more one spaces\n",
    "        y= re.sub(r\"&(amp;)\", \"&\", y) # removes and in html format\n",
    "        y = re.sub(r\"[0-9]\",\"\",y) #remove numbers\n",
    "        y=re.sub(r\"(.+?)\\1+\",r\"\\1\",y) #remove repeted letters\n",
    "        y= re.sub(\"\\s+\",\" \",y) #remove more one space\n",
    "\n",
    "        i = y\n",
    "        new_text.append(i)\n",
    "        \n",
    "    dataframe[\"text\"] = new_text\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = removal_function(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, Remove Stopwords, Lemmatize, Stemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataframe(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_text = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        i = i.lower()\n",
    "        i = RegexpTokenizer(r'\\w+').tokenize(i)\n",
    "        new_text.append(i)\n",
    "        \n",
    "    text_column = new_text\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = tokenize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_dataframe(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_words = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_text = [j for j in i if not j in stop_words]\n",
    "        new_words.append(stop_text)\n",
    "    \n",
    "    text_column = new_words\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = remove_stopwords_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_dataframe(dataframe):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_words = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        lemma = [wordnet.lemmatize(token) for token in i]\n",
    "        new_words.append(lemma)\n",
    "        \n",
    "    text_column = new_words\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = lemmatize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmatize_dataframe(dataframe):\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_words= []\n",
    "    \n",
    "    for i in text_column:\n",
    "        stemmed = [stemmer.stem(token) for token in i]\n",
    "        new_words.append(stemmed)\n",
    "    \n",
    "    text_column = new_words\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = stemmatize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize_dataframe(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_text = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        i = \" \".join(i)\n",
    "        new_text.append(i)\n",
    "        \n",
    "    text_column = new_text\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = untokenize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_textblob(text):\n",
    "    analyze = TextBlob(text)\n",
    "\n",
    "    if analyze.polarity == 0.0:\n",
    "        sentiment_value = \"neutral\"\n",
    "        return sentiment_value, analyze.polarity\n",
    "    elif analyze.polarity < 0.0:\n",
    "        sentiment_value = \"negative\"\n",
    "        return sentiment_value, analyze.polarity\n",
    "    elif analyze.polarity > 0.0:\n",
    "        sentiment_value = \"positive\"\n",
    "        return sentiment_value, analyze.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_textblob(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    sentiment_textblob_list = []\n",
    "\n",
    "    for i in text_column:\n",
    "        sentiment_value, polarity = s.sentiment_textblob(i)#sentiment_textblob(i)\n",
    "        sentiment_textblob_list.append(sentiment_value)\n",
    "\n",
    "    dataframe[\"sentiment_textblob\"] = sentiment_textblob_list\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = add_sentiment_textblob(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_nltk(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    sentiment_nltk_list = []\n",
    "\n",
    "    for i in text_column:\n",
    "        sentiment_value, polarity = s.sentiment_nltk(i)#sentiment_textblob(i)\n",
    "        sentiment_nltk_list.append(sentiment_value)\n",
    "\n",
    "    dataframe[\"sentiment_nltk\"] = sentiment_nltk_list\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = add_sentiment_nltk(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment own trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_own(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    sentiment_own_list = []\n",
    "\n",
    "    for i in text_column:\n",
    "        sentiment_value, polarity = s.sentiment(i)#sentiment_textblob(i)\n",
    "        if polarity < 0.75:\n",
    "            sentiment_own_list.append(\"neutral\")\n",
    "        else:\n",
    "            sentiment_own_list.append(sentiment_value)\n",
    "\n",
    "    dataframe[\"sentiment_own_classifiers\"] = sentiment_own_list\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = add_sentiment_own(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph sentiment & BTC time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "example_tweets_graph = copy.deepcopy(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "      <th>Language</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "      <th>sentiment_nltk</th>\n",
       "      <th>sentiment_own_classifiers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>bitcoin unit price market cap hr vol</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>learn s bitcoin lowest averag daili price chan...</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>仮想通貨の時価総額 btc 価格 ドミナンス eth 価格 ドミナンス ixt 位 価格 ド...</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>iam platform curat retwet via twiter com _alte...</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 00:00:01</td>\n",
       "      <td>bitcoin btc price chang h market cap rank bitc...</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime                                               text  \\\n",
       "0  2019-01-01 00:00:00               bitcoin unit price market cap hr vol   \n",
       "1  2019-01-01 00:00:00  learn s bitcoin lowest averag daili price chan...   \n",
       "2  2019-01-01 00:00:01  仮想通貨の時価総額 btc 価格 ドミナンス eth 価格 ドミナンス ixt 位 価格 ド...   \n",
       "3  2019-01-01 00:00:01  iam platform curat retwet via twiter com _alte...   \n",
       "4  2019-01-01 00:00:01  bitcoin btc price chang h market cap rank bitc...   \n",
       "\n",
       "  Language sentiment_textblob sentiment_nltk sentiment_own_classifiers  \n",
       "0       en            neutral        neutral                       neg  \n",
       "1       en            neutral       negative                   neutral  \n",
       "2       en            neutral        neutral                       neg  \n",
       "3       en           positive        neutral                       neg  \n",
       "4       en           negative        neutral                       neg  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tweets_graph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "example_tweets_graph[\"datetime\"] = pd.to_datetime(example_tweets_graph[\"datetime\"])\n",
    "example_tweets_graph = example_tweets_graph.set_index(\"datetime\")\n",
    "example_tweets_graph[\"hour\"] = example_tweets_graph.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Language</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "      <th>sentiment_nltk</th>\n",
       "      <th>sentiment_own_classifiers</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>bitcoin unit price market cap hr vol</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>learn s bitcoin lowest averag daili price chan...</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:01</th>\n",
       "      <td>仮想通貨の時価総額 btc 価格 ドミナンス eth 価格 ドミナンス ixt 位 価格 ド...</td>\n",
       "      <td>en</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:01</th>\n",
       "      <td>iam platform curat retwet via twiter com _alte...</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:01</th>\n",
       "      <td>bitcoin btc price chang h market cap rank bitc...</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "datetime                                                                 \n",
       "2019-01-01 00:00:00               bitcoin unit price market cap hr vol   \n",
       "2019-01-01 00:00:00  learn s bitcoin lowest averag daili price chan...   \n",
       "2019-01-01 00:00:01  仮想通貨の時価総額 btc 価格 ドミナンス eth 価格 ドミナンス ixt 位 価格 ド...   \n",
       "2019-01-01 00:00:01  iam platform curat retwet via twiter com _alte...   \n",
       "2019-01-01 00:00:01  bitcoin btc price chang h market cap rank bitc...   \n",
       "\n",
       "                    Language sentiment_textblob sentiment_nltk  \\\n",
       "datetime                                                         \n",
       "2019-01-01 00:00:00       en            neutral        neutral   \n",
       "2019-01-01 00:00:00       en            neutral       negative   \n",
       "2019-01-01 00:00:01       en            neutral        neutral   \n",
       "2019-01-01 00:00:01       en           positive        neutral   \n",
       "2019-01-01 00:00:01       en           negative        neutral   \n",
       "\n",
       "                    sentiment_own_classifiers  hour  \n",
       "datetime                                             \n",
       "2019-01-01 00:00:00                       neg     0  \n",
       "2019-01-01 00:00:00                   neutral     0  \n",
       "2019-01-01 00:00:01                       neg     0  \n",
       "2019-01-01 00:00:01                       neg     0  \n",
       "2019-01-01 00:00:01                       neg     0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tweets_graph.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnn_counts_textblob = example_tweets_graph.groupby([\"hour\", \"sentiment_textblob\"])[\"text\"].count()\n",
    "pnn_counts_textblob = pnn_counts_textblob.to_frame()\n",
    "pnn_counts_textblob = pnn_counts_textblob.reset_index()\n",
    "pnn_counts_textblob = pnn_counts_textblob.rename(columns= {\"text\":\"textblob_count\"})\n",
    "\n",
    "pnn_counts_nltk = example_tweets_graph.groupby([\"hour\", \"sentiment_nltk\"])[\"text\"].count()\n",
    "pnn_counts_nltk = pnn_counts_nltk.to_frame()\n",
    "pnn_counts_nltk = pnn_counts_nltk.reset_index()\n",
    "pnn_counts_nltk = pnn_counts_nltk.rename(columns= {\"text\":\"nltk_count\"})\n",
    "\n",
    "pnn_counts_own = example_tweets_graph.groupby([\"hour\", \"sentiment_own_classifiers\"])[\"text\"].count()\n",
    "pnn_counts_own = pnn_counts_own.to_frame()\n",
    "pnn_counts_own = pnn_counts_own.reset_index()\n",
    "pnn_counts_own = pnn_counts_own.rename(columns= {\"text\":\"own_classifier_count\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>nltk_count</th>\n",
       "      <th>textblob_count</th>\n",
       "      <th>own_classifier_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>65</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour sentiment  nltk_count  textblob_count  own_classifier_count\n",
       "0     0  negative           7               4                    76\n",
       "1     0   neutral          65              69                    13\n",
       "2     0  positive          22              21                     5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textblob_count = pnn_counts_textblob[\"textblob_count\"].tolist()\n",
    "own_classifier_count = pnn_counts_own[\"own_classifier_count\"].tolist()\n",
    "pnn_counts_nltk[\"textblob_count\"] = textblob_count\n",
    "pnn_counts_nltk[\"own_classifier_count\"] = own_classifier_count\n",
    "\n",
    "pnn_counts = pnn_counts_nltk\n",
    "pnn_counts = pnn_counts.rename(columns= {\"sentiment_nltk\":\"sentiment\"})\n",
    "pnn_counts.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/matplotlib/figure.py:1498: UserWarning: sharex argument to subplots() was an integer. Did you intend to use subplot() (without 's')?\n",
      "  \"sharex argument to subplots() was an integer. \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "sharex [3] must be one of ['all', 'row', 'col', 'none']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-57bfe34b3661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hour\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"textblob_count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpnn_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hour\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nltk_count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpnn_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[1;32m   1218\u001b[0m     axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n\u001b[1;32m   1219\u001b[0m                        \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_kw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m                        gridspec_kw=gridspec_kw)\n\u001b[0m\u001b[1;32m   1221\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(self, nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             raise ValueError(\"sharex [%s] must be one of %s\" %\n\u001b[0;32m-> 1502\u001b[0;31m                              (sharex, share_values))\n\u001b[0m\u001b[1;32m   1503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msharey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshare_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             raise ValueError(\"sharey [%s] must be one of %s\" %\n",
      "\u001b[0;31mValueError\u001b[0m: sharex [3] must be one of ['all', 'row', 'col', 'none']"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3240x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,2,3, figsize=(45,15))\n",
    "sns.lineplot(x=\"hour\", y=\"textblob_count\", hue=\"sentiment\", data=pnn_counts, ax=ax[0])\n",
    "sns.lineplot(x=\"hour\", y=\"nltk_count\", hue=\"sentiment\", data=pnn_counts, ax=ax[1])\n",
    "sns.lineplot(x=\"hour\", y=\"own_classifier_count\", hue=\"sentiment\", data=pnn_counts, ax=ax[2])\n",
    "#plt.legend(loc=\"upper left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
