{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        \n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "short_pos = codecs.open(\"short_reviews/positive.txt\",\"r\",encoding=\"latin2\").read()\n",
    "short_neg = codecs.open(\"short_reviews/negative.txt\",\"r\", encoding=\"latin2\").read()\n",
    "\n",
    "short_pos = open(\"pos_tweets_415.txt\",\"r\", encoding=\"ISO-8859-1\").read()\n",
    "short_neg = open(\"neg_tweets_415.txt\",\"r\", encoding=\"ISO-8859-1\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pos = open(\"pos_tweets_415.txt\",\"r\", encoding=\"ISO-8859-1\").read()\n",
    "short_neg = open(\"neg_tweets_415.txt\",\"r\", encoding=\"ISO-8859-1\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lines = [line.rstrip('\\n') for line in open('Tagged tweets/pos_tagged.txt', 'r', encoding='ISO-8859-1')]\n",
    "neg_lines = [line.rstrip('\\n') for line in open('Tagged tweets/neg_tagged.txt', 'r', encoding='ISO-8859-1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "documents = []\n",
    "\n",
    "for r in pos.split(\"\\n\"):\n",
    "    documents.append((r,\"pos\"))\n",
    "\n",
    "for r in neg.split(\"\\n\"):\n",
    "    documents.append((r,\"neg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to select only adjects = J, adverbs = R and verb = V we delted all the other words in the text.\n",
    "\n",
    "allowed_word_types = [\"J\"]\n",
    "\n",
    "for p in pos_lines:\n",
    "    documents.append((p,\"pos\"))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n",
    "\n",
    "for p in neg_lines:\n",
    "    documents.append((p,\"neg\"))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_documents = open(\"pickled_algorithms/own_labeled/documents.pickle\",\"wb\")\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_words = []\n",
    "\n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bitcoin', 117), ('ly', 95), ('ã¢', 50), ('new', 47), ('crypto', 46), ('top', 32), ('btc', 28), ('com', 22), ('last', 19), ('usd', 17), ('bulish', 17), ('gl', 17), ('next', 17), ('algory', 17), ('financial', 16), ('coin', 16), ('wil', 15), ('u', 15), ('stelar', 14), ('low', 14), ('major', 14), ('daily', 13), ('first', 13), ('digital', 13), ('short', 12), ('riple', 12), ('uk', 12), ('ap', 12), ('dolar', 11), ('great', 11), ('eth', 10), ('etf', 10), ('update', 10), ('right', 10), ('technical', 10), ('best', 9), ('fre', 9), ('strong', 9), ('global', 9), ('much', 9), ('least', 9), ('sel', 8), ('al', 8), ('january', 8), ('ready', 8), ('future', 8), ('stret', 8), ('net', 8), ('god', 8), ('moderate', 8), ('ltc', 7), ('smart', 7), ('real', 7), ('long', 7), ('regulatory', 7), ('n', 7), ('ful', 7), ('sv', 7), ('posible', 7), ('pic', 7), ('average', 7), ('se', 7), ('invest', 6), ('indian', 6), ('togle', 6), ('bad', 6), ('due', 6), ('ico', 6), ('important', 6), ('win', 6), ('dlvr', 6), ('total', 6), ('south', 6), ('fine', 6), ('high', 6), ('private', 6), ('wrong', 6), ('big', 6), ('profitable', 5), ('invite', 5), ('second', 5), ('th', 5), ('dead', 5), ('able', 5), ('itã¢', 5), ('xrp', 5), ('token', 5), ('worth', 5), ('bear', 5), ('diferent', 5), ('hot', 5), ('several', 5), ('united', 5), ('public', 5), ('launch', 5), ('sucesful', 5), ('many', 5), ('utm_medium', 5), ('social', 5), ('tech', 5)]\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(100))\n",
    "print(all_words[\"bitcoin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#commonly used words to train against\n",
    "word_features = list(all_words.keys())[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_word_features = open(\"pickled_algorithms/own_labeled/word_features.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print((find_features(movie_reviews.words(\"neg/cv000_29416.txt\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_featuresets = open(\"pickled_algorithms/featuresets.pickle\",\"wb\")\n",
    "pickle.dump(featuresets, save_featuresets)\n",
    "save_featuresets.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = featuresets[:600]\n",
    "testing_set = featuresets[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy: 61.73913043478261\n",
      "Most Informative Features\n",
      "                    blog = True              pos : neg    =      6.5 : 1.0\n",
      "                    bear = True              neg : pos    =      6.2 : 1.0\n",
      "                   right = True              neg : pos    =      5.5 : 1.0\n",
      "                    join = True              pos : neg    =      5.1 : 1.0\n",
      "                    much = True              neg : pos    =      4.9 : 1.0\n",
      "                   first = True              pos : neg    =      4.5 : 1.0\n",
      "                   alert = True              pos : neg    =      4.5 : 1.0\n",
      "                   short = True              neg : pos    =      4.2 : 1.0\n",
      "                     low = True              neg : pos    =      4.2 : 1.0\n",
      "                   forex = True              pos : neg    =      3.8 : 1.0\n",
      "                    info = True              pos : neg    =      3.8 : 1.0\n",
      "                   worst = True              neg : pos    =      3.6 : 1.0\n",
      "              utm_medium = True              neg : pos    =      3.6 : 1.0\n",
      "                     buf = True              neg : pos    =      3.6 : 1.0\n",
      "                    tech = True              pos : neg    =      3.5 : 1.0\n",
      "                 digital = True              pos : neg    =      3.5 : 1.0\n",
      "                   stret = True              pos : neg    =      3.1 : 1.0\n",
      "                  monero = True              pos : neg    =      3.1 : 1.0\n",
      "                    bakt = True              pos : neg    =      3.1 : 1.0\n",
      "                     wal = True              pos : neg    =      3.1 : 1.0\n",
      "                 fintech = True              pos : neg    =      3.1 : 1.0\n",
      "                      en = True              neg : pos    =      2.9 : 1.0\n",
      "                   canot = True              neg : pos    =      2.9 : 1.0\n",
      "                 monthly = True              neg : pos    =      2.9 : 1.0\n",
      "                    scam = True              neg : pos    =      2.9 : 1.0\n",
      "                jpmorgan = True              neg : pos    =      2.9 : 1.0\n",
      "                moderate = True              neg : pos    =      2.9 : 1.0\n",
      "                    coin = True              pos : neg    =      2.7 : 1.0\n",
      "                coindesk = True              neg : pos    =      2.6 : 1.0\n",
      "              blockchain = True              pos : neg    =      2.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# likelyhood = prior occurences x likihood / evidence\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "#classifier_f = open(\"naivebayes.pickle\", \"rb\")\n",
    "#classifier = pickle.load(classifier_f)\n",
    "#classifier_f.close()\n",
    "\n",
    "print(\"Original Naive Bayes Algo accuracy:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_classifier = open(\"pickled_algorithms/own_labeled/originalnaivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy: 61.30434782608696\n"
     ]
    }
   ],
   "source": [
    "# Mulitnominal classifier\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "#MNB_classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_MNB_classifier = open(\"pickled_algorithms/own_labeled/MNB.pickle\",\"wb\")\n",
    "pickle.dump(MNB_classifier, save_MNB_classifier)\n",
    "save_MNB_classifier.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Gaussian classifier\n",
    "GaussianNB_classifier = SklearnClassifier(GaussianNB())\n",
    "GaussianNB_classifier.train(training_set)\n",
    "print(\"GaussianNB_classifier accuracy:\", (nltk.classify.accuracy(GaussianNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB_classifier accuracy: 60.0\n"
     ]
    }
   ],
   "source": [
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_BernoulliNB_classifier = open(\"pickled_algorithms/own_labeled/BernoulliNB.pickle\",\"wb\")\n",
    "pickle.dump(BernoulliNB_classifier, save_BernoulliNB_classifier)\n",
    "save_BernoulliNB_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy: 60.43478260869565\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_LogisticRegression_classifier = open(\"pickled_algorithms/own_labeled/LogisticRegression.pickle\",\"wb\")\n",
    "pickle.dump(LogisticRegression_classifier, save_LogisticRegression_classifier)\n",
    "save_LogisticRegression_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_SGDClassifier_classifier = open(\"pickled_algorithms/own_labeled/SGDClassifier.pickle\",\"wb\")\n",
    "pickle.dump(SGDClassifier_classifier, save_SGDClassifier_classifier)\n",
    "save_SGDClassifier_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC_classifier accuracy: 48.26086956521739\n"
     ]
    }
   ],
   "source": [
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_SVC_classifier = open(\"pickled_algorithms/own_labeled/SVC.pickle\",\"wb\")\n",
    "pickle.dump(SVC_classifier, save_SVC_classifier)\n",
    "save_SVC_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy: 61.30434782608696\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_LinearSVC_classifier = open(\"pickled_algorithms/own_labeled/LinearSVC.pickle\",\"wb\")\n",
    "pickle.dump(LinearSVC_classifier, save_LinearSVC_classifier)\n",
    "save_LinearSVC_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy: 58.69565217391305\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_NuSVC_classifier = open(\"pickled_algorithms/own_labeled/NuSVC.pickle\",\"wb\")\n",
    "pickle.dump(NuSVC_classifier, save_NuSVC_classifier)\n",
    "save_NuSVC_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vote classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  LogisticRegression_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy: 63.47826086956522\n"
     ]
    }
   ],
   "source": [
    "print(\"voted_classifier accuracy:\", (nltk.classify.accuracy(voted_classifier,testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in testing_set[5][0].items():\n",
    "    if value == True:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    \n",
    "    return voted_classifier.classify(feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling, how to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_classifier = open(\"naivebayes.pickle\", \"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
