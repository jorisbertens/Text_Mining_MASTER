{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from langdetect import detect\n",
    "import re\n",
    "import string\n",
    "#from project_functions import *\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import sentiment_mod_own_labeled as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/jovyan/work/2_Semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_tweets = open(\"/home/jovyan/work/2_Semester/tweets_jan2019\", \"rb\")\n",
    "example_tweets = pickle.load(pickle_tweets)\n",
    "example_tweets = example_tweets[[\"datetime\",\"text\"]]\n",
    "example_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = example_tweets.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter on English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_english(dataframe):\n",
    "    blanco = \"blanco\"\n",
    "    text_column = dataframe[\"text\"]\n",
    "    \n",
    "    # Create a list saving all the languages of the tweets\n",
    "    language_list =[]\n",
    "\n",
    "    for i in text_column:\n",
    "\n",
    "        try:\n",
    "            language = detect(i)\n",
    "            language_list.append(language)\n",
    "        except:\n",
    "            language_list.append(blanco)    \n",
    "    \n",
    "    dataframe[\"Language\"] = language_list\n",
    "    \n",
    "    return dataframe.loc[dataframe['Language'] == \"en\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = filter_english(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removal_function(dataframe):\n",
    "    new_text = []\n",
    "    text_column = dataframe[\"text\"]\n",
    "    for i in text_column:\n",
    "        y = i\n",
    "\n",
    "        y = re.sub(r\"@[A-Z-a-z-0-9_.]+\",\"\", y) #remove users with@\n",
    "        y = y.replace(\"\\n\",\" \") # remove enters\n",
    "        y= re.sub(r\"http\\S+\",\"\",y) # removes links\n",
    "        y= re.sub(\"\\s+\",\" \",y)  #removes more one spaces\n",
    "        y= re.sub(r\"&(amp;)\", \"&\", y) # removes and in html format\n",
    "        y = re.sub(r\"[0-9]\",\"\",y) #remove numbers\n",
    "        y=re.sub(r\"(.+?)\\1+\",r\"\\1\",y) #remove repeted letters\n",
    "        y= re.sub(\"\\s+\",\" \",y) #remove more one space\n",
    "\n",
    "        i = y\n",
    "        new_text.append(i)\n",
    "        \n",
    "    dataframe[\"text\"] = new_text\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = removal_function(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, Remove Stopwords, Lemmatize, Stemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataframe(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_text = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        i = i.lower()\n",
    "        i = RegexpTokenizer(r'\\w+').tokenize(i)\n",
    "        new_text.append(i)\n",
    "        \n",
    "    text_column = new_text\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = tokenize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_dataframe(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_words = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_text = [j for j in i if not j in stop_words]\n",
    "        new_words.append(stop_text)\n",
    "    \n",
    "    text_column = new_words\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = remove_stopwords_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_dataframe(dataframe):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_words = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        lemma = [wordnet.lemmatize(token) for token in i]\n",
    "        new_words.append(lemma)\n",
    "        \n",
    "    text_column = new_words\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweets = lemmatize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmatize_dataframe(dataframe):\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_words= []\n",
    "    \n",
    "    for i in text_column:\n",
    "        stemmed = [stemmer.stem(token) for token in i]\n",
    "        new_words.append(stemmed)\n",
    "    \n",
    "    text_column = new_words\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweets = stemmatize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize_dataframe(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    new_text = []\n",
    "    \n",
    "    for i in text_column:\n",
    "        i = \" \".join(i)\n",
    "        new_text.append(i)\n",
    "        \n",
    "    text_column = new_text\n",
    "    dataframe[\"text\"] = text_column\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = untokenize_dataframe(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_textblob(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    sentiment_textblob_list = []\n",
    "\n",
    "    for i in text_column:\n",
    "        sentiment_value, polarity = s.sentiment_textblob(i)#sentiment_textblob(i)\n",
    "        sentiment_textblob_list.append(sentiment_value)\n",
    "\n",
    "    dataframe[\"sentiment_textblob\"] = sentiment_textblob_list\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = add_sentiment_textblob(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_nltk(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    sentiment_nltk_list = []\n",
    "\n",
    "    for i in text_column:\n",
    "        sentiment_value, polarity = s.sentiment_nltk(i)#sentiment_textblob(i)\n",
    "        sentiment_nltk_list.append(sentiment_value)\n",
    "\n",
    "    dataframe[\"sentiment_nltk\"] = sentiment_nltk_list\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweets = add_sentiment_nltk(example_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment own trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_df_sentiment = open(\"df_tweets/df_nltk.pickle\",\"rb\")\n",
    "df_sentiment = pickle.load(pickle_df_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_own(dataframe):\n",
    "    text_column = dataframe[\"text\"]\n",
    "    sentiment_own_list = []\n",
    "\n",
    "    for i in text_column:\n",
    "        sentiment_value, confidence = s.sentiment(i)#sentiment_textblob(i)\n",
    "        print(i)\n",
    "        if confidence < 0.7 and confidence > -0.7 :\n",
    "            sentiment_own_list.append(\"neutral\")\n",
    "        else:\n",
    "            sentiment_own_list.append(sentiment_value)\n",
    "\n",
    "    dataframe[\"sentiment_own_classifiers\"] = sentiment_own_list\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = add_sentiment_own(df_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph sentiment & BTC time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "example_tweets_graph = copy.deepcopy(example_tweets)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweets_graph.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "example_tweets_graph[\"datetime\"] = pd.to_datetime(example_tweets_graph[\"datetime\"])\n",
    "example_tweets_graph = example_tweets_graph.set_index(\"datetime\")\n",
    "example_tweets_graph[\"hour\"] = example_tweets_graph.index.hour"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweets_graph.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pnn_counts_textblob = example_tweets_graph.groupby([\"hour\", \"sentiment_textblob\"])[\"text\"].count()\n",
    "pnn_counts_textblob = pnn_counts_textblob.to_frame()\n",
    "pnn_counts_textblob = pnn_counts_textblob.reset_index()\n",
    "pnn_counts_textblob = pnn_counts_textblob.rename(columns= {\"text\":\"textblob_count\"})\n",
    "\n",
    "pnn_counts_nltk = example_tweets_graph.groupby([\"hour\", \"sentiment_nltk\"])[\"text\"].count()\n",
    "pnn_counts_nltk = pnn_counts_nltk.to_frame()\n",
    "pnn_counts_nltk = pnn_counts_nltk.reset_index()\n",
    "pnn_counts_nltk = pnn_counts_nltk.rename(columns= {\"text\":\"nltk_count\"})\n",
    "\n",
    "pnn_counts_own = example_tweets_graph.groupby([\"hour\", \"sentiment_own_classifiers\"])[\"text\"].count()\n",
    "pnn_counts_own = pnn_counts_own.to_frame()\n",
    "pnn_counts_own = pnn_counts_own.reset_index()\n",
    "pnn_counts_own = pnn_counts_own.rename(columns= {\"text\":\"own_classifier_count\"})\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "textblob_count = pnn_counts_textblob[\"textblob_count\"].tolist()\n",
    "own_classifier_count = pnn_counts_own[\"own_classifier_count\"].tolist()\n",
    "pnn_counts_nltk[\"textblob_count\"] = textblob_count\n",
    "pnn_counts_nltk[\"own_classifier_count\"] = own_classifier_count\n",
    "\n",
    "pnn_counts = pnn_counts_nltk\n",
    "pnn_counts = pnn_counts.rename(columns= {\"sentiment_nltk\":\"sentiment\"})\n",
    "pnn_counts.head(9)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,2,3, figsize=(45,15))\n",
    "sns.lineplot(x=\"hour\", y=\"textblob_count\", hue=\"sentiment\", data=pnn_counts, ax=ax[0])\n",
    "sns.lineplot(x=\"hour\", y=\"nltk_count\", hue=\"sentiment\", data=pnn_counts, ax=ax[1])\n",
    "sns.lineplot(x=\"hour\", y=\"own_classifier_count\", hue=\"sentiment\", data=pnn_counts, ax=ax[2])\n",
    "#plt.legend(loc=\"upper left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
