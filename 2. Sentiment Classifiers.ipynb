{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class that is created to get the voted classifier.\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        \n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "# import the IMDB reviews that are labeled\n",
    "pos_lines_imdb = codecs.open(\"short_reviews/positive.txt\",\"r\",encoding=\"latin2\").read()\n",
    "neg_lines_imdb = codecs.open(\"short_reviews/negative.txt\",\"r\", encoding=\"latin2\").read()\n",
    "\n",
    "# import the own labeled tweets\n",
    "pos_lines_own = [line.rstrip('\\n') for line in open('Tagged tweets/pos_tagged.txt', 'r', encoding='ISO-8859-1')]\n",
    "neg_lines_own = [line.rstrip('\\n') for line in open('Tagged tweets/neg_tagged.txt', 'r', encoding='ISO-8859-1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_imdb = []\n",
    "documents_imdb = []\n",
    "\n",
    "all_words_own = []\n",
    "documents_own = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to select only adjects = J, adverbs = R and verb = V we delted all the other words in the text.\n",
    "# Add the sentiment to the lines and append to documents_imdb and documents_own.\n",
    "# only select the words that are in selected word types.\n",
    "# append these to all_words_imdb and all_words_own\n",
    "\n",
    "allowed_word_types = [\"J\"]\n",
    "\n",
    "for p in pos_lines_imdb.split(\"\\n\"):\n",
    "    documents_imdb.append((p,\"pos\"))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words_imdb.append(w[0].lower())\n",
    "\n",
    "for p in neg_lines_imdb.split(\"\\n\"):\n",
    "    documents_imdb.append((p,\"neg\"))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words_imdb.append(w[0].lower())\n",
    "            \n",
    "for p in pos_lines_own:\n",
    "    documents_own.append((p,\"pos\"))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words_own.append(w[0].lower())\n",
    "\n",
    "for p in neg_lines_own:\n",
    "    documents_own.append((p,\"neg\"))\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words_own.append(w[0].lower())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#save to a pickle\n",
    "save_documents = open(\"pickled_algorithms/own_labeled/documents.pickle\",\"wb\")\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('good', 369), ('more', 331), ('little', 265), ('funny', 245), ('much', 234), ('bad', 234), ('best', 208), ('new', 206), ('own', 185), ('many', 183), ('most', 167), ('other', 167), ('great', 160), ('big', 156), ('few', 139), ('first', 133), ('real', 132), ('i', 122), ('better', 116), ('full', 116), ('such', 114), ('romantic', 112), ('american', 110), ('old', 106), ('same', 103), ('original', 100), ('human', 100), ('hard', 98), ('[', 97), ('interesting', 97), ('young', 93), ('enough', 92), ('emotional', 89), ('least', 87), ('long', 83), ('last', 82), ('cinematic', 82), ('true', 75), ('entertaining', 75), ('high', 71), ('special', 70), ('predictable', 70), (']', 69), ('visual', 69), ('familiar', 63), ('whole', 63), ('comic', 63), ('enjoyable', 62), ('sweet', 60), ('narrative', 60), ('less', 60), ('short', 59), ('worst', 59), ('strong', 58), ('only', 58), ('fascinating', 53), ('obvious', 52), ('solid', 52), ('powerful', 51), ('modern', 51), ('french', 51), ('rare', 50), ('fresh', 50), ('easy', 50), ('right', 50), ('recent', 49), ('next', 49), ('dramatic', 49), ('dull', 49), ('worth', 48), ('fine', 48), ('sure', 47), ('serious', 47), ('black', 46), ('beautiful', 45), ('small', 45), ('dark', 45), ('hilarious', 44), ('different', 43), ('smart', 43), ('sad', 43), ('impossible', 43), ('classic', 43), ('compelling', 42), ('personal', 42), ('complex', 41), ('psychological', 41), ('slow', 41), ('intelligent', 41), ('pretentious', 41), ('difficult', 40), ('subject', 40), ('flat', 40), ('social', 39), ('likely', 39), ('political', 39), ('clever', 38), ('perfect', 38), ('quirky', 38), ('simple', 37)]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# see the most common words in the imdb labeled data\n",
    "all_words_imdb = nltk.FreqDist(all_words_imdb)\n",
    "print(all_words_imdb.most_common(100))\n",
    "print(all_words_imdb[\"bitcoin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bitcoin', 117), ('ly', 95), ('ã¢', 50), ('new', 47), ('crypto', 46), ('top', 32), ('btc', 28), ('com', 22), ('last', 19), ('usd', 17), ('bulish', 17), ('gl', 17), ('next', 17), ('algory', 17), ('financial', 16), ('coin', 16), ('wil', 15), ('u', 15), ('stelar', 14), ('low', 14), ('major', 14), ('daily', 13), ('first', 13), ('digital', 13), ('short', 12), ('riple', 12), ('uk', 12), ('ap', 12), ('dolar', 11), ('great', 11), ('eth', 10), ('etf', 10), ('update', 10), ('right', 10), ('technical', 10), ('best', 9), ('fre', 9), ('strong', 9), ('global', 9), ('much', 9), ('least', 9), ('sel', 8), ('al', 8), ('january', 8), ('ready', 8), ('future', 8), ('stret', 8), ('net', 8), ('god', 8), ('moderate', 8), ('ltc', 7), ('smart', 7), ('real', 7), ('long', 7), ('regulatory', 7), ('n', 7), ('ful', 7), ('sv', 7), ('posible', 7), ('pic', 7), ('average', 7), ('se', 7), ('invest', 6), ('indian', 6), ('togle', 6), ('bad', 6), ('due', 6), ('ico', 6), ('important', 6), ('win', 6), ('dlvr', 6), ('total', 6), ('south', 6), ('fine', 6), ('high', 6), ('private', 6), ('wrong', 6), ('big', 6), ('profitable', 5), ('invite', 5), ('second', 5), ('th', 5), ('dead', 5), ('able', 5), ('itã¢', 5), ('xrp', 5), ('token', 5), ('worth', 5), ('bear', 5), ('diferent', 5), ('hot', 5), ('several', 5), ('united', 5), ('public', 5), ('launch', 5), ('sucesful', 5), ('many', 5), ('utm_medium', 5), ('social', 5), ('tech', 5)]\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "# see the most common words in the own labeled data\n",
    "all_words_own = nltk.FreqDist(all_words_own)\n",
    "print(all_words_own.most_common(100))\n",
    "print(all_words_own[\"bitcoin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#commonly used words to train against, use the most common words to train on.\n",
    "word_features_imdb = list(all_words_imdb.keys())[:5000]\n",
    "\n",
    "word_features_own = list(all_words_own.keys())[:5000]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_word_features = open(\"pickled_algorithms/own_labeled/word_features.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two functions to get the features\n",
    "def find_features_imdb(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features_imdb:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def find_features_own(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features_own:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the featuresets of imdb and own\n",
    "featuresets_imdb = [(find_features_imdb(rev), category) for (rev, category) in documents_imdb]\n",
    "\n",
    "featuresets_own = [(find_features_own(rev), category) for (rev, category) in documents_own]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shuffle these feature sets so we can randomly select a training set and test set\n",
    "random.shuffle(featuresets_imdb)\n",
    "random.shuffle(featuresets_own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training and test sets\n",
    "training_set_imdb = featuresets_imdb[:10000]\n",
    "testing_set_imdb = featuresets_imdb[10000:]\n",
    "\n",
    "training_set_own = featuresets_own[:600]\n",
    "testing_set_own = featuresets_own[600:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_classifier = open(\"pickled_train_test/training_set_imdb.pickle\",\"wb\")\n",
    "pickle.dump(training_set_imdb, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_classifier = open(\"pickled_train_test/training_set_own.pickle\",\"wb\")\n",
    "pickle.dump(training_set_own, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_classifier = open(\"pickled_train_test/testing_set_imdb.pickle\",\"wb\")\n",
    "pickle.dump(testing_set_imdb, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "save_classifier = open(\"pickled_train_test/testing_set_own.pickle\",\"wb\")\n",
    "pickle.dump(testing_set_own, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy: 57.826086956521735\n",
      "Most Informative Features\n",
      "                    bear = True              neg : pos    =      6.7 : 1.0\n",
      "                    join = True              pos : neg    =      6.6 : 1.0\n",
      "                    scam = True              neg : pos    =      4.8 : 1.0\n",
      "                     xlm = True              pos : neg    =      4.5 : 1.0\n",
      "                   alert = True              pos : neg    =      4.5 : 1.0\n",
      "                   short = True              neg : pos    =      4.1 : 1.0\n",
      "                     buf = True              neg : pos    =      4.1 : 1.0\n",
      "                      th = True              pos : neg    =      3.8 : 1.0\n",
      "                    best = True              pos : neg    =      3.8 : 1.0\n",
      "                    info = True              pos : neg    =      3.8 : 1.0\n",
      "                   first = True              pos : neg    =      3.7 : 1.0\n",
      "                    know = True              neg : pos    =      3.6 : 1.0\n",
      "                     fal = True              neg : pos    =      3.5 : 1.0\n",
      "                   right = True              neg : pos    =      3.2 : 1.0\n",
      "                 digital = True              pos : neg    =      3.1 : 1.0\n",
      "                   mater = True              pos : neg    =      3.1 : 1.0\n",
      "                     key = True              pos : neg    =      3.1 : 1.0\n",
      "                     fun = True              pos : neg    =      3.1 : 1.0\n",
      "                   togle = True              pos : neg    =      3.1 : 1.0\n",
      "                     eos = True              pos : neg    =      3.1 : 1.0\n",
      "                    lead = True              pos : neg    =      3.1 : 1.0\n",
      "                      ai = True              pos : neg    =      3.1 : 1.0\n",
      "              utm_medium = True              neg : pos    =      2.9 : 1.0\n",
      "                 monthly = True              neg : pos    =      2.9 : 1.0\n",
      "                    firm = True              neg : pos    =      2.9 : 1.0\n",
      "                   least = True              neg : pos    =      2.9 : 1.0\n",
      "                   worst = True              neg : pos    =      2.9 : 1.0\n",
      "                    much = True              neg : pos    =      2.5 : 1.0\n",
      "                    node = True              pos : neg    =      2.4 : 1.0\n",
      "                hardware = True              pos : neg    =      2.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# likelyhood = prior occurences x likihood / evidence\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set_own)\n",
    "\n",
    "print(\"Original Naive Bayes Algo accuracy:\", (nltk.classify.accuracy(classifier, testing_set_own))*100)\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_classifier = open(\"pickled_algorithms/own_labeled/originalnaivebayes.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy: 58.26086956521739\n"
     ]
    }
   ],
   "source": [
    "# Mulitnominal classifier\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set_own)\n",
    "print(\"MNB_classifier accuracy:\", (nltk.classify.accuracy(MNB_classifier, testing_set_own))*100)\n",
    "#MNB_classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_MNB_classifier = open(\"pickled_algorithms/own_labeled/MNB.pickle\",\"wb\")\n",
    "pickle.dump(MNB_classifier, save_MNB_classifier)\n",
    "save_MNB_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB_classifier accuracy: 56.52173913043478\n"
     ]
    }
   ],
   "source": [
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set_own)\n",
    "print(\"BernoulliNB_classifier accuracy:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set_own))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_BernoulliNB_classifier = open(\"pickled_algorithms/own_labeled/BernoulliNB.pickle\",\"wb\")\n",
    "pickle.dump(BernoulliNB_classifier, save_BernoulliNB_classifier)\n",
    "save_BernoulliNB_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy: 58.69565217391305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set_own)\n",
    "print(\"LogisticRegression_classifier accuracy:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set_own))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_LogisticRegression_classifier = open(\"pickled_algorithms/own_labeled/LogisticRegression.pickle\",\"wb\")\n",
    "pickle.dump(LogisticRegression_classifier, save_LogisticRegression_classifier)\n",
    "save_LogisticRegression_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy: 54.78260869565217\n"
     ]
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set_own)\n",
    "print(\"SGDClassifier_classifier accuracy:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set_own))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_SGDClassifier_classifier = open(\"pickled_algorithms/own_labeled/SGDClassifier.pickle\",\"wb\")\n",
    "pickle.dump(SGDClassifier_classifier, save_SGDClassifier_classifier)\n",
    "save_SGDClassifier_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC_classifier accuracy: 46.95652173913044\n"
     ]
    }
   ],
   "source": [
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set_own)\n",
    "print(\"SVC_classifier accuracy:\", (nltk.classify.accuracy(SVC_classifier, testing_set_own))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_SVC_classifier = open(\"pickled_algorithms/own_labeled/SVC.pickle\",\"wb\")\n",
    "pickle.dump(SVC_classifier, save_SVC_classifier)\n",
    "save_SVC_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy: 60.43478260869565\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set_own)\n",
    "print(\"LinearSVC_classifier accuracy:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set_own))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_LinearSVC_classifier = open(\"pickled_algorithms/own_labeled/LinearSVC.pickle\",\"wb\")\n",
    "pickle.dump(LinearSVC_classifier, save_LinearSVC_classifier)\n",
    "save_LinearSVC_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy: 63.47826086956522\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set_own)\n",
    "print(\"NuSVC_classifier accuracy:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set_own))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_NuSVC_classifier = open(\"pickled_algorithms/own_labeled/NuSVC.pickle\",\"wb\")\n",
    "pickle.dump(NuSVC_classifier, save_NuSVC_classifier)\n",
    "save_NuSVC_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vote classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  SGDClassifier_classifier,\n",
    "                                  LogisticRegression_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy: 60.0\n"
     ]
    }
   ],
   "source": [
    "print(\"voted_classifier accuracy:\", (nltk.classify.accuracy(voted_classifier,testing_set_own))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
